We	consider	the	fully	automated	recognition	of	actions	in	uncontrolled	environment	Most	existing	work	relies	on	domain	knowledge	to	construct	complex	handcrafted	features	from	inputs	In	addition	the	environments	are	usually	assumed	to	be	controlled	Convolu-	tional	neural	networks	(CNNs)	are	a	type	of	deep	models	that	can	act	directly	on	the	raw	inputs	thus	automating	the	process	of	fea-	ture	construction	However	such	models	are	currently	limited	to	handle	2D	inputs	In	this	paper	we	develop	a	novel	3D	CNN	model	for	action	recognition	This	model	extracts	fea-	tures	from	both	spatial	and	temporal	dimen-	sions	by	performing	3D	convolutions	thereby	capturing	the	motion	information	encoded	in	multiple	adjacent	frames	The	developed	model	generates	multiple	channels	of	infor-	mation	from	the	input	frames	and	the	final	feature	representation	is	obtained	by	com-	bining	information	from	all	channels	We	apply	the	developed	model	to	recognize	hu-	man	actions	in	real-world	environment	and	it	achieves	superior	performance	without	re-	lying	on	handcrafted	features	1	Introduction	Recognizing	human	actions	in	real-world	environment	finds	applications	in	a	variety	of	domains	including	in-	telligent	video	surveillance	customer	attributes	and	shopping	behavior	analysis	However	accurate	recog-	nition	of	actions	is	a	highly	challenging	task	due	to	Appearing	in	Proceedings	of	the	27	th	International	Confer-	ence	on	Machine	Learning	Haifa	Israel	2010	Copyright	2010	by	the	author(s)/owner(s)	95014	USA	cluttered	backgrounds	occlusions	and	viewpoint	vari-	ations	etc	Therefore	most	of	the	existing	approaches	(Efros	et	al	2003	Schu ̈ldt	et	al	2004	Dolla ́r	et	al	2005	Laptev	&	P ́erez	2007	Jhuang	et	al	2007)	make	certain	assumptions	(e	g	small	scale	and	view-	point	changes)	about	the	circumstances	under	which	the	video	was	taken	However	such	assumptions	sel-	dom	hold	in	real-world	environment	In	addition	most	of	these	approaches	follow	the	conventional	paradigm	of	pattern	recognition	which	consists	of	two	steps	in	which	the	first	step	computes	complex	handcrafted	fea-	tures	from	raw	video	frames	and	the	second	step	learns	classifiers	based	on	the	obtained	features	In	real-world	scenarios	it	is	rarely	known	which	features	are	impor-	tant	for	the	task	at	hand	since	the	choice	of	feature	is	highly	problem-dependent	Especially	for	human	ac-	tion	recognition	different	action	classes	may	appear	dramatically	different	in	terms	of	their	appearances	and	motion	patterns	Deep	learning	models	(Fukushima	1980	LeCun	et	al	1998	Hinton	&	Salakhutdinov	2006	Hinton	et	al	2006	Bengio	2009)	are	a	class	of	machines	that	can	learn	a	hierarchy	of	features	by	building	high-level	features	from	low-level	ones	thereby	automating	the	process	of	feature	construction	Such	learning	ma-	chines	can	be	trained	using	either	supervised	or	un-	supervised	approaches	and	the	resulting	systems	have	been	shown	to	yield	competitive	performance	in	visual	object	recognition	(LeCun	et	al	1998	Hinton	et	al	2006	Ranzato	et	al	2007	Lee	et	al	2009a)	natu-	ral	language	processing	(Collobert	&	Weston	2008)	and	audio	classification	(Lee	et	al	2009b)	tasks	The	convolutional	neural	networks	(CNNs)	(LeCun	et	al	1998)	are	a	type	of	deep	models	in	which	trainable	filters	and	local	neighborhood	pooling	operations	are	applied	alternatingly	on	the	raw	input	images	result-	ing	in	a	hierarchy	of	increasingly	complex	features	It	has	been	shown	that	when	trained	with	appropri-	3D	Convolutional	Neural	Networks	for	Human	Action	Recognition	ate	regularization	(Ahmed	et	al	2008	Yu	et	al	2008	Mobahi	et	al	2009)	CNNs	can	achieve	superior	per-	formance	on	visual	object	recognition	tasks	without	relying	on	handcrafted	features	In	addition	CNNs	have	been	shown	to	be	relatively	insensitive	to	certain	variations	on	the	inputs	(LeCun	et	al	2004)	As	a	class	of	attractive	deep	models	for	automated	fea-	ture	construction	CNNs	have	been	primarily	applied	on	2D	images	In	this	paper	we	consider	the	use	of	CNNs	for	human	action	recognition	in	videos	A	sim-	ple	approach	in	this	direction	is	to	treat	video	frames	as	still	images	and	apply	CNNs	to	recognize	actions	at	the	individual	frame	level	Indeed	this	approach	has	been	used	to	analyze	the	videos	of	developing	embryos	(Ning	et	al	2005)	However	such	approach	does	not	consider	the	motion	information	encoded	in	multiple	contiguous	frames	To	effectively	incorporate	the	motion	information	in	video	analysis	we	propose	to	perform	3D	convolution	in	the	convolutional	layers	of	CNNs	so	that	discriminative	features	along	both	spatial	and	temporal	dimensions	are	captured	We	show	that	by	applying	multiple	distinct	convolutional	operations	at	the	same	location	on	the	input	multi-	ple	types	of	features	can	be	extracted	Based	on	the	proposed	3D	convolution	a	variety	of	3D	CNN	archi-	tectures	can	be	devised	to	analyze	video	data	We	develop	a	3D	CNN	architecture	that	generates	multi-	ple	channels	of	information	from	adjacent	video	frames	and	performs	convolution	and	subsampling	separately	in	each	channel	The	final	feature	representation	is	obtained	by	combining	information	from	all	channels	An	additional	advantage	of	the	CNN-based	models	is	that	the	recognition	phase	is	very	efficient	due	to	their	feed-forward	nature	We	evaluated	the	developed	3D	CNN	model	on	the	TREC	Video	Retrieval	Evaluation	(TRECVID)	data1	which	consist	of	surveillance	video	data	recorded	in	London	Gatwick	Airport	We	constructed	a	multi-	module	event	detection	system	which	includes	3D	CNN	as	a	module	and	participated	in	three	tasks	of	the	TRECVID	2009	Evaluation	for	Surveillance	Event	Detection	Our	system	achieved	the	best	performance	on	all	three	participated	tasks	To	provide	indepen-	dent	evaluation	of	the	3D	CNN	model	we	report	its	performance	on	the	TRECVID	2008	development	set	in	this	paper	We	also	present	results	on	the	KTH	data	as	published	performance	for	this	data	is	avail-	able	Our	experiments	show	that	the	developed	3D	CNN	model	outperforms	other	baseline	methods	on	the	TRECVID	data	and	it	achieves	competitive	per-	formance	on	the	KTH	data	without	depending	on	against-all	linear	SVM	is	learned	for	each	action	class	Specifically	we	extract	dense	SIFT	descriptors	(Lowe	2004)	from	raw	gray	images	or	motion	edge	history	images	(MEHI)	(Yang	et	al	2009)	Local	features	on	raw	gray	images	preserve	the	appearance	information	while	MEHI	concerns	with	the	shape	and	motion	pat-	terns	These	SIFT	descriptors	are	calculated	every	6	pixels	from	7	×	7	and	16	×	16	local	image	patches	in	the	same	cubes	as	in	the	3D	CNN	model	Then	they	are	softly	quantized	using	a	512-word	codebook	to	build	the	BoW	features	To	exploit	the	spatial	layout	in-	formation	we	employ	similar	approach	as	the	spatial	pyramid	matching	(SPM)	(Lazebnik	et	al	2006)	to	partition	the	candidate	region	into	2	×	2	and	3	×	4	cells	and	concatenate	their	BoW	features	The	dimension-	ality	of	the	entire	feature	vector	is	512×(2×2+3×4)	=	8192	We	denote	the	method	based	on	gray	images	as	SPMcube	and	the	one	based	on	MEHI	as	SPMcube	gray	MEHI	We	report	the	5-fold	cross-validation	results	in	which	the	data	for	a	single	day	are	used	as	a	fold	The	per-	formance	measures	we	used	are	precision	recall	and	area	under	the	ROC	curve	(ACU)	at	multiple	values	of	FALSE	positive	rates	(FPR)	The	performance	of	the	four	methods	is	summarized	in	Table	2	We	can	observe	from	Table	2	that	the	3D	CNN	model	outperforms	the	frame-based	2D	CNN	model	SPMcube	and	SPMcube	gray	MEHI	significantly	on	the	action	classes	CellToEar	and	Ob-	jectPut	in	all	cases	For	the	action	class	Pointing	3D	CNN	model	achieves	slightly	worse	performance	than	the	other	three	methods	From	Table	1	we	can	see	that	the	number	of	positive	samples	in	the	Pointing	class	is	significantly	larger	than	those	of	the	other	two	classes	Hence	we	can	conclude	that	the	3D	CNN	model	is	more	effective	when	the	number	of	positive	samples	is	small	Overall	the	3D	CNN	model	outperforms	other	three	methods	consistently	as	can	be	seen	from	the	average	performance	in	Table	2	4	2	Action	Recognition	on	KTH	Data	We	evaluate	the	3D	CNN	model	on	the	KTH	data	(Schu ̈ldt	et	al	2004)	which	consist	of	6	action	classes	performed	by	25	subjects	To	follow	the	setup	in	the	HMAX	model	we	use	a	9-frame	cube	as	input	and	ex-	tract	foreground	as	in	(Jhuang	et	al	2007)	To	reduce	the	memory	requirement	the	resolutions	of	the	input	frames	are	reduced	to	80	×	60	in	our	experiments	as	compared	to	160	×	120	used	in	(Jhuang	et	al	2007)	We	use	a	similar	3D	CNN	architecture	as	in	Figure	3	with	the	sizes	of	kernels	and	the	number	of	feature	maps	in	each	layer	modified	to	consider	the	80	×	60	×	9	inputs	In	particular	the	three	convolutional	layers	use	kernels	of	sizes	9×7	7×7	and	6×4	respec-	tively	and	the	two	subsampling	layers	use	kernels	of	size	3×3	By	using	this	setting	the	80×60×9	in-	puts	are	converted	into	128D	feature	vectors	The	final	layer	consists	of	6	units	corresponding	to	the	6	classes	As	in	(Jhuang	et	al	2007)	we	use	the	data	for	16	ran-	domly	selected	subjects	for	training	and	the	data	for	the	other	9	subjects	for	testing	The	recognition	per-	formance	averaged	across	5	random	trials	is	reported	in	Table	3	along	with	published	results	in	the	litera-	ture	The	3D	CNN	model	achieves	an	overall	accu-	racy	of	90	2%	as	compared	with	91	7%	achieved	by	the	HMAX	model	Note	that	the	HMAX	model	use	handcrafted	features	computed	from	raw	images	with	4-fold	higher	resolution	5	Conclusions	and	Discussions	We	developed	a	3D	CNN	model	for	action	recognition	in	this	paper	This	model	construct	features	from	both	spatial	and	temporal	dimensions	by	performing	3D	convolutions	The	developed	deep	architecture	gener-	ates	multiple	channels	of	information	from	adjacent	in-	put	frames	and	perform	convolution	and	subsampling	separately	in	each	channel	The	final	feature	represen-	tation	is	computed	by	combining	information	from	all	channels	We	evaluated	the	3D	CNN	model	using	the	TRECVID	and	the	KTH	data	sets	Results	show	that	the	3D	CNN	model	outperforms	compared	methods	on	the	TRECVID	data	while	it	achieves	competitive	performance	on	the	KTH	data	demonstrating	its	su-	perior	performance	in	real-world	environments
